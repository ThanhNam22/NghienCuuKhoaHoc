{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2oD0QjDqpUNO"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D\n","from tensorflow.keras.optimizers import SGD , RMSprop, Adam\n","import tensorflow as tf\n","from keras.layers import Conv2D,MaxPooling2D, MaxPool2D, AveragePooling2D , Input\n","from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n","from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import theano\n","from PIL import Image, ImageOps\n","from numpy import *\n","import pickle\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23626,"status":"ok","timestamp":1659016996238,"user":{"displayName":"26.Lã Huy Hoàng","userId":"05470508982825563388"},"user_tz":-420},"id":"uA1uSnDOHqly","outputId":"a3b992c4-e2e6-4ec0-eb86-1bb9a466ee27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"b0PO2yEYgBSN"},"source":["# **Import data Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlVO7tGALk7G"},"outputs":[],"source":["trainingPath = '/content/drive/MyDrive/FruitDataAI/TrainingData'\n","\n","fruitTrain = os.listdir(trainingPath)\n","print(fruitTrain)\n","\n","fruitlist = []\n","\n","for fruit in fruitTrain:\n","  fruitPath = trainingPath +'/'+ fruit\n","  images = os.listdir(fruitPath)\n","  for image in images:\n","    fruitlist.append(array(Image.open(fruitPath + '/' + image).resize((224, 224))))\n","    print(fruitPath + '/' + image)\n","\n","trainingDataSet = array(fruitlist)\n","trainingDataSet.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyURU6RTsev6"},"outputs":[],"source":["# save load data\n","pickle.dump(trainingDataSet, open('/content/drive/MyDrive/FruitDataAI/LoadData/data.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHZIOGqm-4O_"},"outputs":[],"source":["plt.imshow(trainingDataSet[48])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Q2IC7qw_vfs"},"outputs":[],"source":["trainingLabel = []\n","i = 0\n","print(fruitTrain)\n","for fruit in fruitTrain:\n","  fruitPath = trainingPath +'/'+ fruit\n","  images = os.listdir(fruitPath)\n","  for image in images:\n","    trainingLabel.append(i)\n","  i = i + 1\n","trainingLabel = array(trainingLabel)\n","print(trainingLabel)\n","print(trainingLabel.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL6iM3Vttxzt"},"outputs":[],"source":[" pickle.dump(trainingLabel, open('/content/drive/MyDrive/FruitDataAI/LoadData/labels.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdNYfgJ5A3-N"},"outputs":[],"source":["index = 71\n","plt.imshow(trainingDataSet[index])\n","print('This is', fruitTrain[trainingLabel[index]], trainingLabel[index])"]},{"cell_type":"markdown","metadata":{"id":"5xXfzOW9avij"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1557,"status":"ok","timestamp":1659019390552,"user":{"displayName":"26.Lã Huy Hoàng","userId":"05470508982825563388"},"user_tz":-420},"id":"yrJbgfHGvHap","outputId":"b5bcd027-1c45-4709-9a98-cbf343cddd17"},"outputs":[{"name":"stdout","output_type":"stream","text":["(544, 224, 224, 3)\n","(544,)\n"]}],"source":["trainingDataSet = pickle.load(open('/content/drive/MyDrive/FruitDataAI/LoadData/data.pkl','rb'))\n","print(trainingDataSet.shape)\n","trainingLabel = pickle.load(open('/content/drive/MyDrive/FruitDataAI/LoadData/labels.pkl','rb'))\n","print(trainingLabel.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1659019393161,"user":{"displayName":"26.Lã Huy Hoàng","userId":"05470508982825563388"},"user_tz":-420},"id":"YkQetv9tv5Mn","outputId":"a58c39e1-60dd-4f3e-9c24-a764cc5f4555"},"outputs":[{"name":"stdout","output_type":"stream","text":["(544, 150528)\n"]}],"source":["flatten_img = []\n","for img in trainingDataSet:\n","  flatten_img.append(img.flatten())\n","\n","flatten_img = array(flatten_img)\n","print(flatten_img.shape)"]},{"cell_type":"markdown","metadata":{"id":"7I3ayGW9gOjj"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Xs2v0_-ugW-T"},"source":["# **Build Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GByUMItKy4Ng"},"outputs":[],"source":["def VGG16(input_shape ,  num_class):\n","  model = Sequential()\n","\n","  model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=  input_shape ))\n","  model.add(Conv2D(64, kernel_size=3, activation='relu' ))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(128, kernel_size=3, activation='relu'))\n","  model.add(Conv2D(128, kernel_size=3, activation='relu' ))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(256, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(256, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(256, kernel_size=3, activation='relu' ))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  \n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(Conv2D(512, kernel_size=3, activation='relu' ))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","  model.add(Flatten())\n","  model.add(Dense(4096, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(4096, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(num_class , activation = 'softmax'))\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDLiKmeWyy7N"},"outputs":[],"source":["def AlexNet(input_shape , num_class):\n","  model = Sequential()\n","\n","  model.add(Conv2D(filters=96, kernel_size=(11, 11), strides = 4 , activation='relu', input_shape= input_shape))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size = (3,3) ,strides=2))\n","\n","  model.add(Conv2D(filters=256, kernel_size=(5,5) , padding = 'same' , activation='relu'))\n","  model.add(MaxPooling2D(pool_size = (3,3) ,strides=2))\n","\n","  model.add(Conv2D(filters=384, kernel_size=(3,3), padding = 'same', activation='relu'))\n","  model.add(Conv2D(filters=384, kernel_size=(3,3), padding = 'same' , activation='relu'))\n","  model.add(Conv2D(filters=256, kernel_size=(3,3), padding = 'same', activation='relu'))\n","  model.add(MaxPooling2D(pool_size = (3,3) ,strides=2))\n","\n","  model.add(Flatten())\n","  model.add(Dense(4096, activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(4096 , activation='relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(num_class, activation='softmax'))\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySS0pGQqNNB7"},"outputs":[],"source":["def CNN(input_shape, num_of_classes):\n","  model = Sequential()\n","\n","  model.add(Conv2D(32, kernel_size=(3, 3), input_shape = input_shape, padding='same'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Activation('relu'))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Activation('relu'))\n","  model.add(BatchNormalization())\n","\n","  model.add(Conv2D(128, kernel_size=(3, 3), padding='same'))\n","  model.add(MaxPooling2D(pool_size=(2, 2)))\n","  model.add(Dropout(0.5))\n","\n","  model.add(Flatten())\n","  model.add(Dense(256, activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dense(num_of_classes, activation='softmax'))\n","  \n","  model.compile(loss='sparse_categorical_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","  \n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BquiA-Hw-tK5","outputId":"f3b90b89-f0b1-4de4-80af-f4f69756d255"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 224, 224, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 112, 112, 32)     0         \n"," )                                                               \n","                                                                 \n"," activation (Activation)     (None, 112, 112, 32)      0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 112, 112, 32)     128       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 56, 56, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," activation_1 (Activation)   (None, 56, 56, 64)        0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 56, 56, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 56, 56, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 28, 28, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," dropout (Dropout)           (None, 28, 28, 128)       0         \n","                                                                 \n"," flatten (Flatten)           (None, 100352)            0         \n","                                                                 \n"," dense (Dense)               (None, 256)               25690368  \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_1 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 25,787,080\n","Trainable params: 25,786,376\n","Non-trainable params: 704\n","_________________________________________________________________\n","Epoch 1/30\n","55/55 [==============================] - 57s 998ms/step - loss: 1.9321 - accuracy: 0.4207\n","Epoch 2/30\n","55/55 [==============================] - 56s 1s/step - loss: 1.2761 - accuracy: 0.5632\n","Epoch 3/30\n","55/55 [==============================] - 55s 995ms/step - loss: 0.9666 - accuracy: 0.6667\n","Epoch 4/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.9592 - accuracy: 0.6667\n","Epoch 5/30\n","55/55 [==============================] - 55s 994ms/step - loss: 0.5692 - accuracy: 0.8023\n","Epoch 6/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.4436 - accuracy: 0.8552\n","Epoch 7/30\n","55/55 [==============================] - 55s 994ms/step - loss: 0.3389 - accuracy: 0.8989\n","Epoch 8/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.3368 - accuracy: 0.8805\n","Epoch 9/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.2092 - accuracy: 0.9379\n","Epoch 10/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2226 - accuracy: 0.9287\n","Epoch 11/30\n","55/55 [==============================] - 55s 997ms/step - loss: 0.2530 - accuracy: 0.9149\n","Epoch 12/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2412 - accuracy: 0.9264\n","Epoch 13/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1940 - accuracy: 0.9356\n","Epoch 14/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1951 - accuracy: 0.9287\n","Epoch 15/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1790 - accuracy: 0.9494\n","Epoch 16/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1147 - accuracy: 0.9563\n","Epoch 17/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1435 - accuracy: 0.9540\n","Epoch 18/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2926 - accuracy: 0.9103\n","Epoch 19/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.3253 - accuracy: 0.8897\n","Epoch 20/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.5035 - accuracy: 0.8299\n","Epoch 21/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.2014 - accuracy: 0.9333\n","Epoch 22/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1632 - accuracy: 0.9425\n","Epoch 23/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.2370 - accuracy: 0.9241\n","Epoch 24/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1977 - accuracy: 0.9425\n","Epoch 25/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.4606 - accuracy: 0.8391\n","Epoch 26/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.4793 - accuracy: 0.8437\n","Epoch 27/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2156 - accuracy: 0.9172\n","Epoch 28/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1405 - accuracy: 0.9655\n","Epoch 29/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1598 - accuracy: 0.9471\n","Epoch 30/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1851 - accuracy: 0.9379\n","[0.04171256721019745, 0.9954022765159607]\n","[1.805003046989441, 0.5412843823432922]\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.99.54.54.13/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.99.54.54.13/assets\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 224, 224, 32)      896       \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 112, 112, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," activation_2 (Activation)   (None, 112, 112, 32)      0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 112, 112, 32)     128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 112, 112, 64)      18496     \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 56, 56, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," activation_3 (Activation)   (None, 56, 56, 64)        0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 56, 56, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 56, 56, 128)       73856     \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 28, 28, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 28, 28, 128)       0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 100352)            0         \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               25690368  \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_3 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 25,787,080\n","Trainable params: 25,786,376\n","Non-trainable params: 704\n","_________________________________________________________________\n","Epoch 1/30\n","55/55 [==============================] - 63s 1s/step - loss: 1.8791 - accuracy: 0.4460\n","Epoch 2/30\n","55/55 [==============================] - 57s 1s/step - loss: 1.0584 - accuracy: 0.5701\n","Epoch 3/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.7770 - accuracy: 0.7379\n","Epoch 4/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.4952 - accuracy: 0.8138\n","Epoch 5/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.4684 - accuracy: 0.8506\n","Epoch 6/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.3136 - accuracy: 0.9057\n","Epoch 7/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.4763 - accuracy: 0.8414\n","Epoch 8/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.6584 - accuracy: 0.7701\n","Epoch 9/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.3316 - accuracy: 0.8943\n","Epoch 10/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.2187 - accuracy: 0.9425\n","Epoch 11/30\n","55/55 [==============================] - 61s 1s/step - loss: 0.2403 - accuracy: 0.9287\n","Epoch 12/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.2065 - accuracy: 0.9402\n","Epoch 13/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1469 - accuracy: 0.9609\n","Epoch 14/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0897 - accuracy: 0.9747\n","Epoch 15/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1464 - accuracy: 0.9563\n","Epoch 16/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.0720 - accuracy: 0.9862\n","Epoch 17/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1169 - accuracy: 0.9609\n","Epoch 18/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1579 - accuracy: 0.9586\n","Epoch 19/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.0819 - accuracy: 0.9701\n","Epoch 20/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.0446 - accuracy: 0.9908\n","Epoch 21/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0424 - accuracy: 0.9908\n","Epoch 22/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0262 - accuracy: 0.9931\n","Epoch 23/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0563 - accuracy: 0.9862\n","Epoch 24/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0534 - accuracy: 0.9862\n","Epoch 25/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.0720 - accuracy: 0.9770\n","Epoch 26/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1077 - accuracy: 0.9701\n","Epoch 27/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1167 - accuracy: 0.9678\n","Epoch 28/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1205 - accuracy: 0.9540\n","Epoch 29/30\n","55/55 [==============================] - 59s 1s/step - loss: 0.3539 - accuracy: 0.8943\n","Epoch 30/30\n","55/55 [==============================] - 60s 1s/step - loss: 0.1748 - accuracy: 0.9448\n","[0.02972995676100254, 0.9885057210922241]\n","[1.7681386470794678, 0.60550457239151]\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.98.85.60.55/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.98.85.60.55/assets\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 224, 224, 32)      896       \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 112, 112, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," activation_4 (Activation)   (None, 112, 112, 32)      0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 112, 112, 32)     128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 112, 112, 64)      18496     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 56, 56, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," activation_5 (Activation)   (None, 56, 56, 64)        0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 56, 56, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 56, 56, 128)       73856     \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 28, 28, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 28, 28, 128)       0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 100352)            0         \n","                                                                 \n"," dense_4 (Dense)             (None, 256)               25690368  \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_5 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 25,787,080\n","Trainable params: 25,786,376\n","Non-trainable params: 704\n","_________________________________________________________________\n","Epoch 1/30\n","55/55 [==============================] - 60s 1s/step - loss: 1.8401 - accuracy: 0.4023\n","Epoch 2/30\n","55/55 [==============================] - 58s 1s/step - loss: 1.0641 - accuracy: 0.6161\n","Epoch 3/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.7502 - accuracy: 0.7126\n","Epoch 4/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.6804 - accuracy: 0.7563\n","Epoch 5/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.4333 - accuracy: 0.8460\n","Epoch 6/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.4707 - accuracy: 0.8506\n","Epoch 7/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.4781 - accuracy: 0.8368\n","Epoch 8/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.3880 - accuracy: 0.8690\n","Epoch 9/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.4448 - accuracy: 0.8552\n","Epoch 10/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.7744 - accuracy: 0.7333\n","Epoch 11/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.7447 - accuracy: 0.7379\n","Epoch 12/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.2995 - accuracy: 0.9195\n","Epoch 13/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.3579 - accuracy: 0.8897\n","Epoch 14/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.4975 - accuracy: 0.8391\n","Epoch 15/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.5618 - accuracy: 0.8345\n","Epoch 16/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.3095 - accuracy: 0.8966\n","Epoch 17/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2584 - accuracy: 0.9149\n","Epoch 18/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.2007 - accuracy: 0.9425\n","Epoch 19/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.2159 - accuracy: 0.9333\n","Epoch 20/30\n","55/55 [==============================] - 55s 998ms/step - loss: 0.1742 - accuracy: 0.9356\n","Epoch 21/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1182 - accuracy: 0.9609\n","Epoch 22/30\n","55/55 [==============================] - 55s 995ms/step - loss: 0.0943 - accuracy: 0.9793\n","Epoch 23/30\n","55/55 [==============================] - 55s 997ms/step - loss: 0.1116 - accuracy: 0.9632\n","Epoch 24/30\n","55/55 [==============================] - 55s 997ms/step - loss: 0.0878 - accuracy: 0.9701\n","Epoch 25/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.0657 - accuracy: 0.9839\n","Epoch 26/30\n","55/55 [==============================] - 55s 999ms/step - loss: 0.0989 - accuracy: 0.9632\n","Epoch 27/30\n","55/55 [==============================] - 55s 997ms/step - loss: 0.2524 - accuracy: 0.9218\n","Epoch 28/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1118 - accuracy: 0.9724\n","Epoch 29/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0735 - accuracy: 0.9816\n","Epoch 30/30\n","55/55 [==============================] - 55s 997ms/step - loss: 0.0642 - accuracy: 0.9839\n","[0.002826587064191699, 1.0]\n","[1.2849074602127075, 0.60550457239151]\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.100.00.60.55/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.100.00.60.55/assets\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_9 (Conv2D)           (None, 224, 224, 32)      896       \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 112, 112, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," activation_6 (Activation)   (None, 112, 112, 32)      0         \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 112, 112, 32)     128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 112, 112, 64)      18496     \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 56, 56, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," activation_7 (Activation)   (None, 56, 56, 64)        0         \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 56, 56, 64)       256       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 56, 56, 128)       73856     \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 28, 28, 128)      0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 28, 28, 128)       0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 100352)            0         \n","                                                                 \n"," dense_6 (Dense)             (None, 256)               25690368  \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 256)              1024      \n"," chNormalization)                                                \n","                                                                 \n"," dense_7 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 25,787,080\n","Trainable params: 25,786,376\n","Non-trainable params: 704\n","_________________________________________________________________\n","Epoch 1/30\n","55/55 [==============================] - 62s 1s/step - loss: 1.8202 - accuracy: 0.4184\n","Epoch 2/30\n","55/55 [==============================] - 57s 1s/step - loss: 1.0806 - accuracy: 0.5977\n","Epoch 3/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.7569 - accuracy: 0.7172\n","Epoch 4/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.6057 - accuracy: 0.7724\n","Epoch 5/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.4021 - accuracy: 0.8575\n","Epoch 6/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.3868 - accuracy: 0.8575\n","Epoch 7/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.2842 - accuracy: 0.8966\n","Epoch 8/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.3426 - accuracy: 0.8874\n","Epoch 9/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.2265 - accuracy: 0.9172\n","Epoch 10/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1640 - accuracy: 0.9563\n","Epoch 11/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1476 - accuracy: 0.9471\n","Epoch 12/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1796 - accuracy: 0.9448\n","Epoch 13/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1400 - accuracy: 0.9609\n","Epoch 14/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.1319 - accuracy: 0.9586\n","Epoch 15/30\n","55/55 [==============================] - 58s 1s/step - loss: 0.1216 - accuracy: 0.9632\n","Epoch 16/30\n","55/55 [==============================] - 57s 1s/step - loss: 0.5667 - accuracy: 0.8069\n","Epoch 17/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.2334 - accuracy: 0.9333\n","Epoch 18/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1309 - accuracy: 0.9586\n","Epoch 19/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0949 - accuracy: 0.9793\n","Epoch 20/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0958 - accuracy: 0.9724\n","Epoch 21/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1534 - accuracy: 0.9471\n","Epoch 22/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1168 - accuracy: 0.9701\n","Epoch 23/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0962 - accuracy: 0.9678\n","Epoch 24/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0722 - accuracy: 0.9816\n","Epoch 25/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.1210 - accuracy: 0.9678\n","Epoch 26/30\n","55/55 [==============================] - 59s 1s/step - loss: 0.2494 - accuracy: 0.9218\n","Epoch 27/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0968 - accuracy: 0.9701\n","Epoch 28/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0771 - accuracy: 0.9839\n","Epoch 29/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0693 - accuracy: 0.9747\n","Epoch 30/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0420 - accuracy: 0.9908\n","[0.003682993119582534, 1.0]\n","[1.4856038093566895, 0.6330274939537048]\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.100.00.63.30/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.100.00.63.30/assets\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_12 (Conv2D)          (None, 224, 224, 32)      896       \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 112, 112, 32)     0         \n"," g2D)                                                            \n","                                                                 \n"," activation_8 (Activation)   (None, 112, 112, 32)      0         \n","                                                                 \n"," batch_normalization_12 (Bat  (None, 112, 112, 32)     128       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 112, 112, 64)      18496     \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 56, 56, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," activation_9 (Activation)   (None, 56, 56, 64)        0         \n","                                                                 \n"," batch_normalization_13 (Bat  (None, 56, 56, 64)       256       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 56, 56, 128)       73856     \n","                                                                 \n"," max_pooling2d_14 (MaxPoolin  (None, 28, 28, 128)      0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_4 (Dropout)         (None, 28, 28, 128)       0         \n","                                                                 \n"," flatten_4 (Flatten)         (None, 100352)            0         \n","                                                                 \n"," dense_8 (Dense)             (None, 256)               25690368  \n","                                                                 \n"," batch_normalization_14 (Bat  (None, 256)              1024      \n"," chNormalization)                                                \n","                                                                 \n"," dense_9 (Dense)             (None, 8)                 2056      \n","                                                                 \n","=================================================================\n","Total params: 25,787,080\n","Trainable params: 25,786,376\n","Non-trainable params: 704\n","_________________________________________________________________\n","Epoch 1/30\n","55/55 [==============================] - 56s 995ms/step - loss: 1.9323 - accuracy: 0.4335\n","Epoch 2/30\n","55/55 [==============================] - 54s 977ms/step - loss: 0.9605 - accuracy: 0.6422\n","Epoch 3/30\n","55/55 [==============================] - 54s 978ms/step - loss: 0.7148 - accuracy: 0.7523\n","Epoch 4/30\n","55/55 [==============================] - 54s 981ms/step - loss: 0.3968 - accuracy: 0.8555\n","Epoch 5/30\n","55/55 [==============================] - 54s 977ms/step - loss: 0.4286 - accuracy: 0.8463\n","Epoch 6/30\n","55/55 [==============================] - 54s 976ms/step - loss: 0.2509 - accuracy: 0.9243\n","Epoch 7/30\n","55/55 [==============================] - 54s 980ms/step - loss: 0.2382 - accuracy: 0.9220\n","Epoch 8/30\n","55/55 [==============================] - 54s 983ms/step - loss: 0.1909 - accuracy: 0.9472\n","Epoch 9/30\n","55/55 [==============================] - 54s 980ms/step - loss: 0.2158 - accuracy: 0.9358\n","Epoch 10/30\n","55/55 [==============================] - 54s 983ms/step - loss: 0.2041 - accuracy: 0.9404\n","Epoch 11/30\n","55/55 [==============================] - 55s 995ms/step - loss: 0.2053 - accuracy: 0.9312\n","Epoch 12/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.5288 - accuracy: 0.8326\n","Epoch 13/30\n","55/55 [==============================] - 55s 993ms/step - loss: 0.4245 - accuracy: 0.8486\n","Epoch 14/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.2155 - accuracy: 0.9381\n","Epoch 15/30\n","55/55 [==============================] - 54s 989ms/step - loss: 0.5105 - accuracy: 0.8509\n","Epoch 16/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.9319 - accuracy: 0.6651\n","Epoch 17/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.6811 - accuracy: 0.7454\n","Epoch 18/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.3229 - accuracy: 0.9037\n","Epoch 19/30\n","55/55 [==============================] - 55s 993ms/step - loss: 0.2408 - accuracy: 0.9358\n","Epoch 20/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.1129 - accuracy: 0.9656\n","Epoch 21/30\n","55/55 [==============================] - 54s 987ms/step - loss: 0.1930 - accuracy: 0.9404\n","Epoch 22/30\n","55/55 [==============================] - 54s 989ms/step - loss: 0.1720 - accuracy: 0.9450\n","Epoch 23/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0929 - accuracy: 0.9679\n","Epoch 24/30\n","55/55 [==============================] - 56s 1s/step - loss: 0.0812 - accuracy: 0.9748\n","Epoch 25/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0723 - accuracy: 0.9839\n","Epoch 26/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.0616 - accuracy: 0.9839\n","Epoch 27/30\n","55/55 [==============================] - 55s 1s/step - loss: 0.1228 - accuracy: 0.9610\n","Epoch 28/30\n","55/55 [==============================] - 55s 998ms/step - loss: 0.2327 - accuracy: 0.9266\n","Epoch 29/30\n","55/55 [==============================] - 55s 999ms/step - loss: 0.1089 - accuracy: 0.9633\n","Epoch 30/30\n","55/55 [==============================] - 55s 996ms/step - loss: 0.0836 - accuracy: 0.9771\n","[0.04767077416181564, 0.9931192398071289]\n","[1.180866003036499, 0.6388888955116272]\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.99.31.63.89/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/MyDrive/FruitDataAI/Model/22.07.model.99.31.63.89/assets\n"]}],"source":["model_history = []\n","training_acc_score = []\n","training_loss_score = []\n","testing_acc_score = []\n","testing_loss_score = []\n","models = []\n","\n","storingPath = '/content/drive/MyDrive/FruitDataAI/Model'\n","\n","img_rows, img_cols = 224, 224\n","input_shape = (img_rows, img_cols, 3)\n","KFoldData = StratifiedKFold(n_splits=5, random_state= 16 , shuffle=True)\n","KFoldData.get_n_splits(trainingDataSet,  trainingLabel)\n","\n","for train_index, test_index in KFoldData.split(trainingDataSet, trainingLabel):\n","  X_Train, X_Test = trainingDataSet[train_index],trainingDataSet[test_index]\n","  Y_Train, Y_Test = trainingLabel[train_index], trainingLabel[test_index]\n","\n","  model = CNN(input_shape, 8)\n"," \n","  model.compile(loss='sparse_categorical_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","  \n","  model.summary()\n","  \n","  model_history.append(model.fit(X_Train, Y_Train, batch_size= 8, epochs=30 , verbose=1))\n","\n","  models.append(model)\n","\n","  training_scores = model.evaluate(X_Train, Y_Train, verbose=0)\n","  print(training_scores)\n","\n","  testing_scores = model.evaluate(X_Test, Y_Test, verbose=0)\n","  print(testing_scores)\n","\n","  model.save(storingPath + '/22.07.model.{:.2f}.{:.2f}'.format(training_scores[1] * 100, testing_scores[1] * 100))\n","\n","  training_acc_score.append(training_scores[1] * 100)\n","  training_loss_score.append(training_scores[0])\n","\n","  testing_acc_score.append(testing_scores[1] * 100)\n","  testing_loss_score.append(testing_scores[0])"]},{"cell_type":"markdown","metadata":{"id":"iQzjA0crgj_z"},"source":["# **Review**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UR_Dc2kK_CxH"},"outputs":[],"source":["train_acc = []\n","test_acc = []\n","fit_times = []\n","score_times = []\n","\n","y_pred = np.argmax(model.predict(X_Train), axis=-1)\n","train_acc.append(metrics.accuracy_score(Y_Train, y_pred) * 100)\n","\n","y_pred = np.argmax(model.predict(X_Test), axis=-1)\n","test_acc.append(metrics.accuracy_score(Y_Test, y_pred) * 100)\n","\n","training_scores = model.evaluate(X_Train, Y_Train, verbose=0)\n","testing_scores = model.evaluate(X_Test, Y_Test, verbose=0)\n","\n","train_acc.append(training_scores[1] * 100)\n","test_acc.append(testing_scores[1] * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiFfYwAJvY80"},"outputs":[],"source":["y_pred = np.argmax(model.predict(trainingDataSet), axis=-1)\n","avg = 'micro'\n","\n","#Model Accuracy: Số mẫu được gán nhãn chính xác/Tổng sỗ mẫu test\n","print(\"Accuracy: \", metrics.accuracy_score(trainingLabel, y_pred))\n","\n","#Model precision: Số mẫu được gán nhãn là đúng chính xác/tổng số mẫu được gán nhãn là đúng\n","print(\"Precision: \", metrics.precision_score(trainingLabel, y_pred, average=avg))\n","\n","#Model recall: Số mẫu được gán nhãn là đúng chính xác/Tổng số mẫu thực sự đúng\n","print(\"Recall: \", metrics.recall_score(trainingLabel, y_pred, average=avg))\n","\n","print(\"F1-Score: \", metrics.f1_score(trainingLabel, y_pred,average=avg))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVhieG3lAZ_i"},"outputs":[],"source":["print('{:13s} {:20s} {:20s}'.format('', 'Training scores', 'Testing scores'))\n","\n","for i in range(5):\n","  print('{:13s} {:10.2f}{:<} {:18.2f}{:<}'.format('Experiment ' + str(i + 1) + ':', training_acc_score[i], '%', testing_acc_score[i], '%'))\n","\n","print('-----------------------------------------------------')\n","print('{:>14s} {:9.2f}% {:18.2f}%'.format('Mean: ', np.mean(training_acc_score), np.mean(testing_acc_score)))\n","print('{:>14s} {:9.2f}% {:18.2f}%'.format('Highest: ', max(training_acc_score), max(testing_acc_score)))\n","print('{:>14s} {:9.2f}% {:18.2f}%'.format('Lowest: ', min(training_acc_score), min(testing_acc_score)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gENZwuTGDcPm"},"outputs":[],"source":["plt.title('Accuracies vs Epochs')\n","plt.plot(model_history[0].history['accuracy'], label='Experiment 1')\n","plt.plot(model_history[1].history['accuracy'], label='Experiment 2')\n","plt.plot(model_history[2].history['accuracy'], label='Experiment 3')\n","plt.plot(model_history[3].history['accuracy'], label='Experiment 4')\n","plt.plot(model_history[4].history['accuracy'], label='Experiment 5')\n","# plt.savefig('/content/drive/MyDrive/My_works/CNN/Stored data/K-Fold CV - Grayscale Model/Images/Accuracies.png')\n","plt.legend()\n","plt.show()\n","\n","plt.title('Losses vs Epochs')\n","plt.plot(model_history[0].history['loss'], label='Experiment 1')\n","plt.plot(model_history[1].history['loss'], label='Experiment 2')\n","plt.plot(model_history[2].history['loss'], label='Experiment 3')\n","plt.plot(model_history[3].history['loss'], label='Experiment 4')\n","plt.plot(model_history[4].history['loss'], label='Experiment 5')\n","# plt.savefig('/content/drive/MyDrive/My_works/CNN/Stored data/K-Fold CV - Grayscale Model/Images/Losses.png')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S8EHm4ZYg1Yr"},"source":["# **Load Data Testing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"di4rFvDh7ILo"},"outputs":[],"source":["testingPath = '/content/drive/MyDrive/FruitDataAI/TestingData'\n","\n","fruitTest = os.listdir(testingPath)\n","print(fruitTest)\n","\n","fruitlist_test = []\n","\n","for fruit in fruitTest:\n","  fruitPath = testingPath +'/'+ fruit\n","  images = os.listdir(fruitPath)\n","  for image in images:\n","    fruitlist_test.append(array(Image.open(fruitPath + '/' + image).resize((224,224))))\n","    print(fruitPath + '/' + image)\n","\n","testingDataSet = array(fruitlist_test)\n","testingDataSet.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeSr0N6ze5sL"},"outputs":[],"source":["pickle.dump(testingDataSet, open('/content/drive/MyDrive/FruitDataAI/LoadData/data_test.pkl', 'wb'))"]},{"cell_type":"markdown","metadata":{"id":"X6EPisDVhDmB"},"source":["## **Test Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MY5JOm_8FhVE"},"outputs":[],"source":["testingDataSet = pickle.load(open('/content/drive/MyDrive/FruitDataAI/LoadData/data_test.pkl','rb'))\n","print(testingDataSet.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5EYaHZbDtl6"},"outputs":[],"source":["model = keras.models.load_model('/content/drive/MyDrive/FruitDataAI/Model/22.07.model.99.33.69.64')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPKzBt0VpnRz"},"outputs":[],"source":["fruitTrain = os.listdir('/content/drive/MyDrive/FruitDataAI/TrainingData')\n","fruitTrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3047MRrcfiZA"},"outputs":[],"source":["index = 163\n","pred = model.predict(testingDataSet[index].reshape(1, 224 , 224, 3))\n","print(np.argmax(pred))\n","print('This is', fruitTrain[np.argmax(pred)] )\n","plt.imshow(testingDataSet[index])"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"NghienCuuKhoaHoc","provenance":[{"file_id":"/v2/external/notebooks/snippets/importing_libraries.ipynb","timestamp":1637054211287}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}