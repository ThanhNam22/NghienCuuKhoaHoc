{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, recall_score, precision_score, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize, scale, StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#some initial setup\n",
    "np.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "np.random.seed(1234)\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_reader(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None, CSVpath=\"CSV Files/CSV-A-1.csv\", col=[1, 2, 6], skiprows=100, max_rows_undamaged=4096,\n",
    "                 max_rows_damaged=131072, batch=256, resample_factor=None):\n",
    "    \n",
    "        # data path\n",
    "        col_list=['damaged data', 'undamaged data']\n",
    "        df = pd.read_csv(CSVpath)\n",
    "        damaged = df[\"damaged data\"]\n",
    "        damaged = damaged.replace('',np.nan)\n",
    "        damaged = damaged.dropna( how=\"any\")\n",
    "        undamaged = df[\"undamaged data\"]\n",
    "        undamaged = undamaged.replace('',np.nan)\n",
    "        undamaged = undamaged.dropna( how=\"any\")\n",
    "\n",
    "        col = col\n",
    "        i=0\n",
    "        j=0\n",
    "        self.x_ud=np.empty((0,3), int)\n",
    "        self.x_d=np.empty((0,3), int)\n",
    "\n",
    "        \n",
    "        # damaged\n",
    "        for dpath in damaged:\n",
    "            print(j, dpath)\n",
    "            self.x_inp_d = np.loadtxt(dpath, skiprows=skiprows, usecols=col, max_rows=max_rows_damaged)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if resample_factor==2:\n",
    "                self.x_inp_d = self.x_inp_d[1::2]\n",
    "            elif resample_factor==1:\n",
    "                self.x_inp_d = self.x_inp_d[::2]\n",
    "            \n",
    "            self.x_d = np.append(self.x_d, self.x_inp_d, axis=0)\n",
    "            j+=1 \n",
    "            \n",
    "            \n",
    "        self.d_rows,_ = self.x_d.shape\n",
    "        self.d_batch = int(self.d_rows/batch)\n",
    "        self.y_d = np.ones(self.d_rows, dtype=int).reshape(self.d_rows, 1)\n",
    "        \n",
    "        \n",
    "        # undamaged\n",
    "        for udpath in undamaged:\n",
    "            print(i, udpath)\n",
    "            self.x_inp_ud = np.loadtxt(udpath, skiprows=skiprows, usecols=col, max_rows=max_rows_undamaged)\n",
    "            \n",
    "\n",
    "            if resample_factor==2:\n",
    "                self.x_inp_ud = self.x_inp_ud[1::2]\n",
    "            elif resample_factor==1:\n",
    "                self.x_inp_ud = self.x_inp_ud[::2]\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.x_ud = np.append(self.x_ud, self.x_inp_ud, axis=0)\n",
    "            i+=1\n",
    "            \n",
    "            \n",
    "        self.x_ud = self.x_ud[:self.d_rows, :]\n",
    "        self.ud_rows,_ = self.x_ud.shape\n",
    "        self.ud_batch = int(self.ud_rows/batch)\n",
    "        self.y_ud = np.zeros(self.ud_rows, dtype=int).reshape(self.ud_rows, 1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # stack\n",
    "        self.x = np.vstack((self.x_ud, self.x_d))\n",
    "        self.y = np.vstack((self.y_ud, self.y_d))\n",
    "        self.xy = np.hstack((self.x, self.y))\n",
    "        self.xy = self.xy.reshape((self.ud_batch+self.d_batch, 1, batch, 4))\n",
    "        self.xy = shuffle(self.xy, random_state=42)\n",
    "        \n",
    "        self.x = self.xy[:, :, :, [0,1,2]]\n",
    "        self.y = self.xy[:, :, :, [3]]\n",
    "        \n",
    "        self.z=[]\n",
    "        for i in range(self.ud_batch+self.d_batch):\n",
    "            self.z = np.append(self.z, self.y[i, 0, batch-1, 0])\n",
    "        self.y = np.reshape(self.z, (self.ud_batch+self.d_batch))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.n_samples = self.x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.tensor(int(targets), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snr_noise:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        \n",
    "        target_snr_db = 5         # Target dB\n",
    "        \n",
    "        x_watts = inputs ** 2\n",
    "        x_db = 10 * np.log10(x_watts)\n",
    "        sig_avg_watts = np.mean(x_watts)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), x_watts.shape)\n",
    "        inputs = inputs + noise_volts\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_noise_tfms = transforms.Compose([\n",
    "    snr_noise(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(model, dataloader):\n",
    "    \"\"\" Predict probabilities for the given model and dataset\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        device = torch.device(run.device)\n",
    "        result = []\n",
    "        y = []\n",
    "        for inputs, targets in tqdm(dataloader):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            scores = model(inputs)\n",
    "            preds = scores.data.sign() / 2 + 0.5\n",
    "            result += [preds.cpu().numpy()]\n",
    "            y += [targets.cpu().numpy()]\n",
    "\n",
    "        result = np.concatenate(result).reshape(-1).astype(int)\n",
    "        y = np.concatenate(y)\n",
    "    return result, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_names(model, dataloader, class_names=['undamaged', 'damaged']):\n",
    "    \"\"\" Predict probabilities for the given model and dataset\n",
    "    \n",
    "        Inputs:\n",
    "            - model: a pytorch model\n",
    "            - dataloader a torch.utils.data.DataLoader object\n",
    "            - class_names: a list of class names\n",
    "            \n",
    "        Output:\n",
    "            - result: Predicted class name for each input as a python list\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        result = []\n",
    "        y = []\n",
    "        model = model.to(device)\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = to_var(inputs)\n",
    "            scores = model(inputs)\n",
    "            preds = scores.data.sign() / 2 + 0.5\n",
    "            result += [preds.cpu().numpy()]\n",
    "            y += [labels.cpu().numpy()]\n",
    "            \n",
    "        result = np.concatenate(result).reshape(-1).astype(int)\n",
    "        y = np.concatenate(y)\n",
    "        \n",
    "        pred_class_names = [class_names[i] for i in result]\n",
    "    print('Damage Possibility: ', 100*sum(result)/len(result), ' %', 'actual damage:', 100*sum(y)/len(y), ' %')\n",
    "    return  pred_class_names, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_DP_AVG(model, dataloader, number=5):\n",
    "    \"\"\" Predict probabilities for the given model and dataset\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    Results = []\n",
    "    since = time.time()\n",
    "    for i in range(number):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            Result = []\n",
    "            result = []\n",
    "            y = []\n",
    "            for inputs, targets in tqdm(dataloader):\n",
    "                inputs = to_var(inputs)\n",
    "                scores = model(inputs)\n",
    "                preds = scores.data.sign() / 2 + 0.5\n",
    "                result += [preds.cpu().numpy()]\n",
    "                y += [targets.cpu().numpy()]\n",
    "        \n",
    "            result = np.concatenate(result).reshape(-1).astype(int)\n",
    "            y = np.concatenate(y)\n",
    "            Result = 100*sum(result)/len(result)\n",
    "        Results.append(Result)\n",
    "    times = time.time() - since\n",
    "    return Results , 'Average DP:', np.average(Results), 'Average Time: ', times/number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_CPU(model, dataloader, number=5):\n",
    "    \"\"\" Predict probabilities for the given model and dataset\n",
    "    \"\"\"\n",
    "    model = model.cpu()\n",
    "    Results = []\n",
    "    since = time.time()\n",
    "    for i in range(number):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            Result = []\n",
    "            result = []\n",
    "            y = []\n",
    "            \n",
    "            for inputs, targets in tqdm(dataloader):\n",
    "                scores = model(inputs)\n",
    "                preds = scores.data.sign() / 2 + 0.5\n",
    "                result += [preds.cpu().numpy()]\n",
    "                y += [targets.cpu().numpy()]\n",
    "        \n",
    "            result = np.concatenate(result).reshape(-1).astype(int)\n",
    "            y = np.concatenate(y)\n",
    "            Result = 100*sum(result)/len(result)\n",
    "        Results.append(Result)\n",
    "    times = time.time() - since\n",
    "    return Results , 'Average DP:', np.average(Results), 'Average Time: ', times/number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HingeLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        L = (1 - target * inputs).clamp(min=0)\n",
    "        return torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_id = 1\n",
    "column=[29,30,25]\n",
    "Signal_Lenth=256\n",
    "\n",
    "#max rows undamaged:\n",
    "max_rows_undamaged_t=17874  #for tripple-damaged cases\n",
    "\n",
    "\n",
    "train_ds = data_reader(transform=tfms, CSVpath=f\"CSV Files/CSV-A-{net_id}.csv\",\n",
    "                       col=column, skiprows=11, max_rows_undamaged=max_rows_undamaged_t, max_rows_damaged=196608,\n",
    "                       batch=Signal_Lenth, resample_factor=None)\n",
    "\n",
    "#max rows undamaged:\n",
    "max_rows_undamaged_v=938   #for single-damaged cases\n",
    "\n",
    "valid_ds = data_reader(transform=tfms, CSVpath=f\"CSV Files/CSV-B-{net_id}.csv\",\n",
    "                       col=column, skiprows=196619, max_rows_undamaged=max_rows_undamaged_v, max_rows_damaged=32768,\n",
    "                       batch=Signal_Lenth, resample_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## resample_factore: 1 for odd lines and 2 for even lines\n",
    "\n",
    "net_id = 30\n",
    "column=[29,30,25]\n",
    "Signal_Lenth=256\n",
    "resampling_factor = 'half'\n",
    "\n",
    "## max rows undamaged:\n",
    "max_rows_undamaged_t=7490   #for single-damaged cases\n",
    "\n",
    "train_ds = data_reader(transform=tfms, CSVpath=f\"CSV Files/CSV-A-{net_id}.csv\",\n",
    "                       col=column, skiprows=11, max_rows_undamaged=max_rows_undamaged_t, max_rows_damaged=262144,\n",
    "                       batch=Signal_Lenth, resample_factor=1)\n",
    "\n",
    "## max rows undamaged:\n",
    "max_rows_undamaged_v=1874   #for single-damaged cases\n",
    "\n",
    "valid_ds = data_reader(transform=tfms, CSVpath=f\"CSV Files/CSV-B-{net_id}.csv\",\n",
    "                       col=column, skiprows=11, max_rows_undamaged=max_rows_undamaged_v, max_rows_damaged=65536,\n",
    "                       batch=Signal_Lenth, resample_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_data = train_ds[0]\n",
    "print(first_data)\n",
    "features, labels = first_data\n",
    "print(features.shape, labels.shape)\n",
    "print(labels)\n",
    "print(type(features), type(labels))\n",
    "plt.plot(features[0, :, 0])\n",
    "plt.xlabel('sample step')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_data = valid_ds[0]\n",
    "print(first_data)\n",
    "features, labels = first_data\n",
    "print(features.shape, labels.shape)\n",
    "print(labels)\n",
    "print(type(features), type(labels))\n",
    "plt.plot(features[0, :, 0])\n",
    "plt.xlabel('smple step')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_SVM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_SVM, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(9,3), padding=(4,1)),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1))\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=(3,3), padding=(1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((4, 1))\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        self.svm = nn.Linear(16*32*3 , 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.svm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [0.01],\n",
    "    device = [\"cuda\"],\n",
    "    batch_size = [8],\n",
    "    epoch_number = [30],\n",
    "    shuffle = [True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in RunBuilder.get_runs(params):    \n",
    "    valid_dl = DataLoader(dataset=valid_ds, batch_size=run.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = HingeLoss()\n",
    "\n",
    "accuracy_list = []\n",
    "accuracy_list_v = []\n",
    "loss_list = []\n",
    "loss_list_v = []\n",
    "with open(f'Loss/losses-svm-{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}.txt', 'w') as fa:\n",
    "    \n",
    "    for run in RunBuilder.get_runs(params):\n",
    "\n",
    "        device = torch.device(run.device)\n",
    "        model = CNN_SVM().double()\n",
    "        network = model.to(device)\n",
    "        loader = DataLoader(train_ds, batch_size=run.batch_size, shuffle=run.shuffle)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=run.lr, weight_decay=1e-5)\n",
    "\n",
    "        scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[4,20,25], gamma=0.1, verbose=True)\n",
    "\n",
    "        \n",
    "        if not os.path.exists(f'wts/svm-{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}'):\n",
    "            os.mkdir(f'wts/svm-{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}')\n",
    "            \n",
    "        \n",
    "        since = time.time()\n",
    "        best_model_wts = network.state_dict()\n",
    "        best_acc = 0.0\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_v = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_acc_v = 0\n",
    "        \n",
    "        for epoch in range(run.epoch_number):\n",
    "            total_loss = 0\n",
    "            total_loss_v=0\n",
    "            \n",
    "            for batch in loader:\n",
    "\n",
    "                inputs = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "                labels = Variable(2 * (labels.float() - 0.5))\n",
    "                outputs = network(inputs)\n",
    "\n",
    "\n",
    "                loss = criterion(outputs.t(), labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            epoch_loss = (total_loss/len(loader))\n",
    "            \n",
    "            print('Epoch: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, run.epoch_number, total_loss/len(loader)))\n",
    "\n",
    "\n",
    "            scheduler2.step()\n",
    "            \n",
    "            \n",
    "        # valid Dataset\n",
    "            correct_v = 0.\n",
    "            total_v = 0.\n",
    "            for batch in valid_dl:\n",
    "                inputs_v = batch[0].to(device)\n",
    "                labels_v = batch[1].to(device)\n",
    "                \n",
    "                labels_test_pred = model(inputs_v)\n",
    "                prediction_v = labels_test_pred.data.sign() / 2 + 0.5\n",
    "                \n",
    "                loss_v = criterion(labels_test_pred.t(), labels)\n",
    "                \n",
    "                total_loss_v += loss_v.item()\n",
    "                \n",
    "                correct_v += (prediction_v.view(-1).long() == labels_v.data).sum()\n",
    "                total_v += inputs_v.shape[0]\n",
    "                \n",
    "                total_loss_v += loss_v.item()\n",
    "            epoch_loss_v = (total_loss_v/len(valid_dl))   \n",
    "        \n",
    "        # Train Dataset \n",
    "            correct = 0.\n",
    "            total = 0.\n",
    "            for batch in loader:\n",
    "                inputs = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "\n",
    "                labels_train_pred = network(inputs)\n",
    "                prediction = labels_train_pred.data.sign() / 2 + 0.5\n",
    "\n",
    "                correct += (prediction.view(-1).long() == labels.data).sum()\n",
    "                total += inputs.shape[0]\n",
    "            \n",
    "            epoch_acc = correct.float() / total\n",
    "            epoch_acc_v = correct_v.float() / total_v  \n",
    "            \n",
    "        # deep copy the model\n",
    "            if epoch_acc_v > best_acc:\n",
    "                best_acc = epoch_acc_v\n",
    "                best_model_wts = network.state_dict().copy()\n",
    "                torch.save(best_model_wts, f\"./wts/svm-{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}/epoch-{epoch}-acc-{best_acc}.pth\")\n",
    "                \n",
    "            print('Train Accuracy: %f  \\t Valid Accuracy: %f \\t%%' % (100 * (correct.float() / total), (100 * (correct_v.float() / total_v))))\n",
    "\n",
    "            \n",
    "            accuracy_list.append(epoch_acc.cpu())\n",
    "            accuracy_list_v.append(epoch_acc_v.cpu())      \n",
    "            loss_list.append(epoch_loss)\n",
    "            loss_list_v.append(epoch_loss_v)\n",
    "            \n",
    "            fa.write(\"\\n\")\n",
    "            fa.write('Train Accuracy: %f  \\t Valid Accuracy: %f \\t%%' % (100 * (correct.float() / total), (100 * (correct_v.float() / total_v))))\n",
    "            fa.write(\"\\t Epoch Loss train: %f\\t Epoch Loss valid: %f\\t\" % (epoch_loss, epoch_loss_v))\n",
    "        times = time.time() - since \n",
    "        print('time: ', times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    device = torch.device(run.device)\n",
    "    model = CNN_SVM().double()\n",
    "    network = model.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "    losses = []\n",
    "    Accuracy_tracker = []\n",
    "\n",
    "checkpoint = torch.load(f'Saved_Model\\\\cnn_svm{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}.pt')\n",
    "network.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_number = checkpoint['epoch']\n",
    "loss_list = checkpoint['losses']\n",
    "loss_list_v = checkpoint['losses_v']\n",
    "accuracy_list = checkpoint['Acc']\n",
    "accuracy_list_v = checkpoint['Acc_v']\n",
    "criterion = checkpoint['criterion'] \n",
    "#network.train()\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(loss_list, '-o')\n",
    "plt.plot(loss_list_v, '-o')\n",
    "plt.legend(['Train', 'Valid'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "#plt.ylim([0, 1])\n",
    "X_axis = range(run.epoch_number)\n",
    "plt.xticks(X_axis)\n",
    "plt.plot(accuracy_list, '-o')\n",
    "plt.plot(accuracy_list_v, '-o')\n",
    "plt.legend(['Train', 'Valid'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.title('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=run.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load = train_dl\n",
    "#load = valid_dl\n",
    "\n",
    "load = [train_dl, valid_dl]\n",
    "acc=[]\n",
    "pre=[]\n",
    "re=[]\n",
    "f=[]\n",
    "au=[]\n",
    "\n",
    "print(f'Network No. {net_id} \\t batch_size: {run.batch_size} \\t signal_lenth: {Signal_Lenth}')\n",
    "\n",
    "for l in load:\n",
    "    \n",
    "    y_pred, y_true = predict_class(network, l)\n",
    "    print(namestr(l,globals()))\n",
    "    #print('\\n', 'Exprience No.', i+1,':')\n",
    "    \n",
    "    report_list=[]\n",
    "    \n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_true=y_true,y_pred=y_pred)\n",
    "    #acc.append(accuracy)\n",
    "    report_list.append(accuracy)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_pred=y_pred, y_true=y_true)\n",
    "    #pre.append(precision)\n",
    "    report_list.append(precision)\n",
    "    print('Precision: %f' % precision)\n",
    "\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_pred=y_pred, y_true=y_true)\n",
    "    #re.append(recall)\n",
    "    report_list.append(recall)\n",
    "    print('Recall: %f' % recall)\n",
    "\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_pred=y_pred, y_true=y_true)\n",
    "    #f.append(f1)\n",
    "    report_list.append(f1)\n",
    "    print('F1 score: %f' % f1)\n",
    "\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_true=y_true, y_score=y_pred)\n",
    "    #au.append(auc)\n",
    "    report_list.append(auc)\n",
    "    print('ROC AUC: %f' % auc, '\\n')\n",
    "    for rep in report_list:\n",
    "        print(rep)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, ['udamaged', 'damaged'], figsize=(5,5), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "torch.save({\n",
    "    'epoch' : run.epoch_number,\n",
    "    'losses' : loss_list,\n",
    "    'losses_v' : loss_list_v,\n",
    "    'Acc' : accuracy_list,\n",
    "    'Acc_v': accuracy_list_v,\n",
    "    'model_state_dict': network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'criterion' : criterion,\n",
    "},f'Saved_Model\\\\cnn_svm{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}-sample_rate-{resampling_factor}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_reader(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None, CSVpath=\"CSV Files/CSV-B-1.csv\", col=[1, 2, 6], skiprows=100, max_rows_undamaged=4096,\n",
    "                 max_rows_damaged=131072, batch=256, state=0, case=0):\n",
    "    \n",
    "        # data path\n",
    "        col_list=['damaged data', 'undamaged data']\n",
    "        df = pd.read_csv(CSVpath)\n",
    "        damaged = df[\"damaged data\"]\n",
    "        damaged = damaged.replace('',np.nan)\n",
    "        damaged = damaged.dropna( how=\"any\")\n",
    "        undamaged = df[\"undamaged data\"]\n",
    "        undamaged = undamaged.replace('',np.nan)\n",
    "        undamaged = undamaged.dropna( how=\"any\")\n",
    "\n",
    "        col = col\n",
    "        i=0\n",
    "        j=0\n",
    "        self.x_ud=np.empty((0,3), int)\n",
    "        self.x_d=np.empty((0,3), int)\n",
    "\n",
    "        if state ==0:\n",
    "            \n",
    "            # undamaged\n",
    "            print(case, undamaged[case])\n",
    "            self.x_ud = np.loadtxt(undamaged[case], skiprows=skiprows, usecols=col, max_rows=max_rows_undamaged)\n",
    "\n",
    "            \n",
    "            self.ud_rows,_ = self.x_ud.shape\n",
    "            self.ud_batch = int(self.ud_rows/batch)\n",
    "            self.y_ud = np.zeros(self.ud_rows, dtype=int).reshape(self.ud_rows, 1)\n",
    "            self.x = self.x_ud\n",
    "            self.y = self.y_ud\n",
    "            self.batch = self.ud_batch\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # damaged\n",
    "            print(case, damaged[case])\n",
    "            self.x_d = np.loadtxt(damaged[case], skiprows=skiprows, usecols=col, max_rows=max_rows_damaged)\n",
    "            \n",
    "            \n",
    "            self.d_rows,_ = self.x_d.shape\n",
    "            self.d_batch = int(self.d_rows/batch)\n",
    "            self.y_d = np.ones(self.d_rows, dtype=int).reshape(self.d_rows, 1)\n",
    "            self.x = self.x_d\n",
    "            self.y = self.y_d\n",
    "            self.batch = self.d_batch\n",
    "        \n",
    "        \n",
    "        # stack\n",
    "        self.xy = np.hstack((self.x, self.y))\n",
    "        self.xy = self.xy.reshape((self.batch, 1, 256, 4))\n",
    "        self.xy = shuffle(self.xy, random_state=42)\n",
    "        \n",
    "        self.x = self.xy[:, :, :, [0,1,2]]\n",
    "        self.y = self.xy[:, :, :, [3]]\n",
    "        \n",
    "        self.z=[]\n",
    "        for i in range(self.batch):\n",
    "            self.z = np.append(self.z, self.y[i, 0, 254, 0])\n",
    "        self.y = np.reshape(self.z, (self.batch))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.n_samples = self.x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#state: 0 for undamaged cases and 1 for damaged\n",
    "\n",
    "for i in range(35):\n",
    "    test_ds = test_reader(transform=tfms, CSVpath=\"CSV Files\\CSV-B-30.csv\", col=[29,30,25], skiprows=196619,\n",
    "                            max_rows_undamaged=62720,\n",
    "                            max_rows_damaged=62720, batch=256, state=0, case=i)\n",
    "    test_dl = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "    predict_class_names(network, test_dl, ['Undamage', 'Damage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class_names(network, valid_dl, ['Undamage', 'Damage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_class_DP_AVG(network, test_dl, number=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_CPU(network, test_dl, number=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = test_ds[0]\n",
    "print(first_data)\n",
    "features, labels = first_data\n",
    "print(features.shape, labels.shape)\n",
    "print(labels)\n",
    "print(type(features), type(labels))\n",
    "plt.plot(features[0, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "torch.save({\n",
    "    'epoch' : run.epoch_number,\n",
    "    'losses' : loss_list,\n",
    "    'losses_v' : loss_list_v,\n",
    "    'Acc' : accuracy_list,\n",
    "    'Acc_v': accuracy_list_v,\n",
    "    'model_state_dict': network.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'criterion' : criterion,\n",
    "},f'Saved_Model\\\\cnn_svm{net_id}-b{run.batch_size}-signal_lenth-{Signal_Lenth}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
